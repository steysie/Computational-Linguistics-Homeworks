{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2 - Anastasia Nikiforova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```В самом низу этого ноутбука - которкие выводы.```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "from string import punctuation\n",
    "import json, os\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [line for line in open('corpus_eng.txt', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "digits = re.compile(\"./d.\") # Getting rid of any strings that contain numbers: time, date etc.\n",
    "arabic = ['i', 'ii', 'iii', 'iv', 'v','vi', 'vii', 'viii', 'xi', 'x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumbers(string):\n",
    "    return any(char.isdigit() for char in string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split() if word]\n",
    "    words = [word for word in words if word and word not in stops]\n",
    "    words = [word for word in words if word and word not in arabic]\n",
    "    words = [word for word in words if word and not hasNumbers(word)]\n",
    "    words = [word for word in words if word and word not in punct]\n",
    "    words = [lemma.lemmatize(word) for word in words if word]\n",
    "    words = [word.replace(\"'s\", '') for word in words if word] \n",
    "    words = [word.replace(\"o'\", '') for word in words if word] \n",
    "    words = [word.strip(punct) for word in words if word]  # just in case (какие-то точки не убрались сразу)\n",
    "    words = [word for word in words if word and len(word)>2]\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'mat',\n",
       " 'riley',\n",
       " 'house',\n",
       " 'hidden',\n",
       " 'wrote',\n",
       " 'done',\n",
       " 'computational',\n",
       " 'linguistics',\n",
       " 'break',\n",
       " 'break',\n",
       " 'break',\n",
       " 'breaking',\n",
       " 'broke',\n",
       " 'broken']"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \". ThE cat's on tHe mat:&? o'riley! will be in a me houses \\\n",
    "        hidden wrote done 123ew 321ouch iii x COmpuTatioNal Linguistics \\\n",
    "        breaks o'break's BREAK breaking broke broken\"\n",
    "normalize(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in corpus_norm:\n",
    "#     if 'breaking' in i:\n",
    "#         print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хм, интересно! Здесь видно, что формы прошедшего времени и герундия лемматизатор почему-то не лемматизировал. Хотя такие предложения в корпусе однозначно есть (код закомменчен, чтобы не отображать длинный список)\n",
    "\n",
    "**[Эта проверка была проведена не в самом начале, а только после того, как были получены 418 предложений, содержащих ```break```. Модификация функции нормализации будет ниже.]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"States to Watch on Election Day 11/7/2016 6:00AM Trump and Clinton will put their election strategies to the test on Tuesday. WSJ's Gerald F. Seib discusses which states to watch as the polls close and which states both candidates need to win in order to claim victory. Photo: AP Transcript \\n\""
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_norm = [normalize(line) for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_norm_1 = [normalize(line) for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['price',\n",
       " 'pledged',\n",
       " 'work',\n",
       " 'ensure',\n",
       " 'health',\n",
       " 'care',\n",
       " 'system',\n",
       " 'work',\n",
       " 'patient',\n",
       " 'family',\n",
       " 'doctor',\n",
       " 'lead',\n",
       " 'world',\n",
       " 'cure',\n",
       " 'prevention',\n",
       " 'illness',\n",
       " 'based',\n",
       " 'sensible',\n",
       " 'rule',\n",
       " 'protect',\n",
       " 'well-being',\n",
       " 'country',\n",
       " 'embracing',\n",
       " 'innovative',\n",
       " 'spirit']"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Избавились от любых токенов, в которых есть цифры (встречались даты, время  т.п.), \n",
    "# в т.ч. арабские (встречалась такая нумерация).\n",
    "# Все тексты лемматизированы с помощью Wordnet Lemmatizer. \n",
    "# Избавились от posessive case (т.к. 's не несет важной для нас информации) и от приставки o'.\n",
    "# Ограничили длину слова (>2), удалена пунктуация и стоп-слова.\n",
    "corpus_norm[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "497"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_break = [w for w in corpus_norm if 'break' in w]\n",
    "len(corpus_break)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "497 - Получилось чуточку больше!*\n",
    "\n",
    "\\* *Без лемматизации было всего 408 включений, а значит в корпусе немало разных форм слова break (breaks/breaking etc.).*\n",
    "\n",
    "P.S. Позже выяснилось, что не все формы лемматизируются как надо, поэтому нормализация была дополнена еще одним условием."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "### Лирическое отступление: Модификация функции нормализации "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы в нормализации не писать несколько строчек реплейса, ввожу функцию замены сразу по нескольким вариантам\n",
    "def replaceMultiple(mainString, toBeReplaces, newString):\n",
    "    # Iterate over the strings to be replaced\n",
    "    for elem in toBeReplaces:\n",
    "        # Check if string is in the main string\n",
    "        if elem in mainString:\n",
    "            # Replace the string\n",
    "            mainString = mainString.replace(elem, newString)\n",
    "    \n",
    "    return  mainString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_forms = ['breaks', 'breaking', 'broke', 'broken']\n",
    "stop_extra = set(list(stops) + ['yes', 'sorry', 'can', 'could', 'would'])  # часто встречаются, но по сути бессмыссленны\n",
    "for_replacement = [\"'s\", \"o'\", \"'m\", \"'ve\", \"'d'\", \"’s\", \"’ve\", \"’d\", \"’m\", \"o’\"] # в корпусе встречается два вида апострофов\n",
    "\n",
    "def normalize_upd(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split() if word]\n",
    "    \n",
    "    # Тут возникла проблема с примерами типа \"else?(sorry\" (нет пробела после знака препинания)\n",
    "    # Решение - разделим каждый подобный пример по знаку пунктуации:\n",
    "    words = [\"\".join((char if char.isalpha() else \" \") for char in word).split() for word in words if word]\n",
    "    # При этом функция isalpha() дополнительно уберет все строки, в которых есть цифры\n",
    "    # Значит наша функция hasNumbers() больше не нужна.\n",
    "    \n",
    "    # Тут еще одна проблема - теперь в списке появятся подсписки из-за разделения проблемных участков на предыдущем шаге\n",
    "    # Нужно \"расправить\" список:\n",
    "    words = [word for sublist in words for word in sublist]\n",
    "    \n",
    "    # Теперь можно нормализовать дальше\n",
    "    words = [lemma.lemmatize(word) for word in words if word]\n",
    "    words = [word for word in words if word and word not in stops]\n",
    "    words = [word for word in words if word and word not in arabic] # но арабские цифры нужно убрать\n",
    "    # words = [word for word in words if word and not hasNumbers(word)] - obsolete\n",
    "    words = [lemma.lemmatize(word) for word in words if word]\n",
    "    words = [replaceMultiple(word, for_replacement, '') for word in words if word] \n",
    "#     words = [word.replace(\"'s\", '') for word in words if word]\n",
    "#     words = [word.replace(\"o'\", '') for word in words if word] \n",
    "    words = [word.strip(punct) for word in words if word]  # just in case (какие-то точки не убрались сразу)\n",
    "    words = [word for word in words if word and len(word)>2]\n",
    "    words = ['break' if word in break_forms else word for word in words if word] # дополняем лемматизацию\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Забавно, что ```word.lstrip(\"o'\")``` и ```word.rstrip(\"'s\")``` просто съедали буквы ```o``` и ```s``` в начале/конце слов, даже при экранировании апострофа (пробовала варианты ```''``` и ```\\'```). Интересно, почему ```strip``` не работает с апострофами...\n",
    "\n",
    "Тем не менее, ```replace``` справился отлично. \n",
    "UPD: лучше написать функцию для мультизамены, т.к. нужно много чего заменить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat',\n",
       " 'mat',\n",
       " 'riley',\n",
       " 'house',\n",
       " 'ouch',\n",
       " 'computational',\n",
       " 'linguistics',\n",
       " 'break',\n",
       " 'break',\n",
       " 'break',\n",
       " 'break',\n",
       " 'break',\n",
       " 'break',\n",
       " 'order',\n",
       " 'mass',\n",
       " 'house',\n",
       " 'load',\n",
       " 'trash']"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \". ThE cat's on tHe mat:&? o'riley! will be in a me houses \\\n",
    "        123ew 321ouch iii x COmpuTatioNal Linguistics \\\n",
    "        breaks o'break's BREAK breaking broke broken order mass’s hi!i'm(house) o’loads)of!trash’d\"\n",
    "normalize_upd(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ух, как красиво теперь все токенизируется! Еще бы лемматизация проходила идеально для всех - было бы прекрасно!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_norm = [normalize_upd(line) for line in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['price', 'pledged', 'work', 'ensure', 'health', 'care', 'system', 'work', 'patient', 'family', 'doctor', 'lead', 'world', 'cure', 'prevention', 'illness', 'based', 'sensible', 'rule', 'protect', 'well', 'country', 'embracing', 'innovative', 'spirit']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_norm[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь текст лемматизирован гораздо лучше. Но, конечно, не идеально, потому что лучше бы лемматизировать все слова как надо, а не только ```break```. Но если лемматизировать ```description```/```examples``` этой же функцией, по идее все должно быть хорошо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1256"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_break = [w for w in corpus_norm if 'break' in w]\n",
    "len(corpus_break)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ух тыы! Почти в три раза возрасло количество предложений! (а то и больше, т.к. без лемматизации было 408)\n",
    "\n",
    "Значит разных форм слова ```break``` было не \"немало\", как было заявлено выше, а даже больше, чем слова ```break``` изначально в начальной форме!\n",
    "\n",
    "Надеюсь, что это поможет нормально, а не по-медвежьи (хоть медведи и довольно милые)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Позже выяснилось, что очень много вариантов \"breaking news\", алгоритм всегда определяет смысл как ```break_61 - be released or become known; of news```, что было бы верно в случае \"break the news\", но с \"breaking news\" это вряд ли 100% подходящее значение.\n",
    "\n",
    "Короче говоря, сделаем отдельный список без брейкинг ньюс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sublist(ls1, ls2):\n",
    "    '''\n",
    "    >>> sublist([], [1,2,3])\n",
    "    True\n",
    "    >>> sublist([1,2,3,4], [2,5,3])\n",
    "    True\n",
    "    >>> sublist([1,2,3,4], [0,3,2])\n",
    "    False\n",
    "    >>> sublist([1,2,3,4], [1,2,5,6,7,8,5,76,4,3])\n",
    "    False\n",
    "    '''\n",
    "    def get_all_in(one, another):\n",
    "        for element in one:\n",
    "            if element in another:\n",
    "                yield element\n",
    "\n",
    "    for x1, x2 in zip(get_all_in(ls1, ls2), get_all_in(ls2, ls1)):\n",
    "        if x1 != x2:\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breaking_news = ['break', 'news']\n",
    "corpus_break_nonews = [w for w in corpus_break if sublist(breaking_news, w)]  # ['break', 'news']\n",
    "len(corpus_break_nonews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На мой взгляд, это тоже не сильно повредит"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(map(len, corpus_break))  # выясним минимальную длину элемента в получившемся корпусе - на всякий случай"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это наверняка не очень полезно для алгоритма... Давайте введем ограничение на то, что в нормализованном предложении должно быть не меньше, скажем, шести слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_break_lim = [del(i) for i in corpus_break if len(i)<6] # так низя\n",
    "corpus_break_lim = [i for i in corpus_break if len(i)>6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1229"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_break_lim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Было 1256, стало 1229 - стало быть, мы избавились от 27 коротких значений. На общем фоне, потеря кажется совсем небольшой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, тсколлько всего значений break содержится в wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'break'\n",
    "# i = 0\n",
    "# for synset in wn.synsets(word):\n",
    "#     print(str(i) + ' - ' + word + ' - ' + synset.definition())\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "75! ой-ой... много :\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word = 'break'\n",
    "# i = 0\n",
    "# for synset in wn.synsets(word):\n",
    "#     print(str(i) + ' - ' + word + ' - ' + ' | '.join(synset.examples()))\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "corpus_break_10 = random.sample(corpus_break_lim, 10)\n",
    "len(corpus_break_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Надо бы еще имена убрать... А то полно всяких Billy Joel-ов и Jordan Philips-ов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_break_10_text = []\n",
    "# temp = \"\"\n",
    "# for i in range(len(corpus_break_10)):\n",
    "#     corpus_break_10_text.append(' '.join(corpus_break_10[i]))\n",
    "#     temp = \"\"\n",
    "\n",
    "# corpus_break_10_text[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_context(words, window=3):\n",
    "    \n",
    "    words_in_context = []\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        left = words[max(0,i-window):i]\n",
    "        right = words[i+1:i+1+window]\n",
    "        target = words[i]\n",
    "        words_in_context.append((target, left+right))\n",
    "        \n",
    "    \n",
    "    return words_in_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попытка номер 1 - поиск по Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_definition( word, sentence ):\n",
    "    \n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    sentence = set(sentence)\n",
    "    \n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        definition = tokenize(synset.definition())\n",
    "        definition = set(definition)\n",
    "        overlap = len(definition & sentence)\n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap = overlap\n",
    "            bestsense = i\n",
    "    \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis_corpus = []\n",
    "# пройдем по корпусу\n",
    "for text in corpus_break_10:\n",
    "    dis_text = []\n",
    "    words_in_context = get_words_in_context(text)\n",
    "    # дизамбигуируем каждой слов\n",
    "    for word, context in words_in_context:\n",
    "        nsense = lesk_definition(word, context)\n",
    "        \n",
    "        # если смысл не нулевой - добавим индекс смысла к токену\n",
    "        if nsense > 0:\n",
    "            dis_text.append(word + '_' + str(nsense))\n",
    "        else:\n",
    "            dis_text.append(word)\n",
    "    \n",
    "    dis_corpus.append(dis_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man', 'shooting', 'burlington', 'county', 'street', 'published', 'saturday', 'november', 'burlington', 'city', 'prosecutor', 'say', 'man', 'shot', 'street', 'burlington', 'county', 'died', 'wound', 'still', 'clear', 'sparked', 'shooting', 'howard', 'young', 'year', 'old', 'burlington', 'township', 'man', 'shot', 'around', 'friday', 'block', 'east', 'federal', 'street_2', 'burlington', 'city', 'taken', 'hospital', 'died', 'short_17', 'time', 'later', 'burlington', 'county', 'prosecutor', 'say', 'autopsy', 'scheduled', 'saturday', 'declined', 'comment', 'slaying', 'citing', 'ongoing', 'investigation', 'arrest', 'made', 'latest', 'sfgate', 'homepage', 'click', 'top', 'news', 'around', 'bay', 'area', 'beyond', 'sign', 'newsletter', 'first', 'learn', 'break_61', 'news', 'sign', 'manage', 'profile', 'top', 'page', 'latest', 'news']\n"
     ]
    }
   ],
   "source": [
    "print(dis_corpus[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt_1 = dis_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что можно попробовать изменить:\n",
    "* смотреть на examples\n",
    "* смотреть на definition+examples\n",
    "* смотреть на нормализованные examples/definitions\n",
    "* изменить размер окна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - break_40',\n",
       " '1 - break',\n",
       " '1 - break',\n",
       " '2 - break',\n",
       " '3 - break_61',\n",
       " '4 - break',\n",
       " '4 - break_61',\n",
       " '5 - break',\n",
       " '6 - break',\n",
       " '7 - break',\n",
       " '8 - break_61',\n",
       " '9 - break_15']"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options_1 = []\n",
    "for i in range(len(attempt_1)):\n",
    "    for j in attempt_1[i]:\n",
    "        if 'break' in j:\n",
    "            options_1.append(str(i) + \" - \" + j)\n",
    "options_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Введем новую функцию, чтобы не писать каждый раз вывод всех вариантов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_all_senses(dis_corp):\n",
    "    options = []\n",
    "    for i in range(len(dis_corp)):\n",
    "        for j in dis_corp[i]:\n",
    "            if j[:5] == 'break':\n",
    "                if len(j) == 5:\n",
    "                    options.append(str(i) + \" - \" + j + \"_0\" + \" - \" +  wn.synsets('break')[0].definition())\n",
    "                else:\n",
    "                    options.append(str(i) + \" - \" + j + \" - \" +  wn.synsets('break')[int(j.split('_')[1])].definition())\n",
    "    return options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И еще одну для дизамбигуации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate(corpus):\n",
    "    dis_corpus = []\n",
    "    # пройдем по корпусу\n",
    "    for text in corpus:\n",
    "        dis_text = []\n",
    "        words_in_context = get_words_in_context(text) \n",
    "        # дизамбигуируем каждой слов\n",
    "        for word, context in words_in_context:\n",
    "            nsense = lesk_definition(word, context)\n",
    "\n",
    "            # если смысл не нулевой - добавим индекс смысла к токену\n",
    "            if nsense > 0:\n",
    "                dis_text.append(word + '_' + str(nsense))\n",
    "            else:\n",
    "                dis_text.append(word)\n",
    "\n",
    "        dis_corpus.append(dis_text)\n",
    "    return dis_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - break_40 - cause to give up a habit',\n",
       " '1 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '1 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '2 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '3 - break_61 - be released or become known; of news',\n",
       " '4 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '4 - break_61 - be released or become known; of news',\n",
       " '5 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '6 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '7 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '8 - break_61 - be released or become known; of news',\n",
       " '9 - break_15 - an escape from jail']"
      ]
     },
     "execution_count": 468,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_all_senses(attempt_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['country', 'prepared', 'election', 'flight', 'lieutenant', 'jerry', 'rawlings', 'several', 'junior', 'officer', 'launched_2', 'coup', 'successful', 'first_14', 'another', 'group', 'officer', 'break_15', 'jail', 'rawlings', 'made_51', 'second', 'successful', 'coup', 'attempt', 'overthrew', 'government']\n"
     ]
    }
   ],
   "source": [
    "print(attempt_1[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты:**\n",
    "```\n",
    "* 0 - Из лемматизированного корпуса не очень понятно, но кажется, тут молодой человек заствил 'kid' что-то бросить - мб верно\n",
    "* 1 - Здесь брейк как каникулы - неверно\n",
    "* 2 - Что-то про музыку - неверно\n",
    "* 3 - Брейкинг ньюс - предположим, что это верно\n",
    "* 4 - Тут есть и брейкинг ньюс (верно) + что-то про каникулы в апреле (неверно)\n",
    "* 5 - Что-то про атаки (неверно)\n",
    "* 6 - Что-то про болезни (неверно)\n",
    "* 7 - Что-то про марши в корее и политическое напряжение (неверно)\n",
    "* 8 - Брейкинг ньюс (верно)\n",
    "* 9 - Побег из тюрьмы! Верно!\n",
    "\n",
    "Итого: 4,5/10 (в 4 примере верно только одно break)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прекрасно! А теперь\n",
    "\n",
    "## Попытка номер 1.1 - поиск по Definitions в корпусе без \"Breaking news\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonews_dis = disambiguate(random.sample(corpus_break_nonews, 10))\n",
    "temp = nonews_dis # чтобы не пропало :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '1 - break_10 - the opening shot that scatters the balls in billiards or pool',\n",
       " '2 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '3 - break_7 - breaking of hard tissue such as bone',\n",
       " \"4 - break_2 - (geology) a crack in the earth's crust resulting from the displacement of one side with respect to the other\",\n",
       " '5 - break_61 - be released or become known; of news',\n",
       " '6 - break_1 - an unexpected piece of good luck',\n",
       " '7 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '8 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '9 - break_0 - some abrupt occurrence that interrupts an ongoing activity']"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_all_senses(nonews_dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perception', 'pelly', 'speaks', 'rooted', 'december', 'incident', 'little', 'india', 'riot', 'break', 'inebriated', 'indian', 'worker', 'died', 'wheel', 'private', 'bus', 'according', 'record', 'people', 'injured', 'arrested']\n"
     ]
    }
   ],
   "source": [
    "print(temp[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Верно**\\*: \n",
    "```\n",
    "* 0 (долгая засуха сменилась резким дождем)\n",
    "* 5 (таки новости пробрались, но все же смысл про выпуск чего-то (в частности новостей))\n",
    "* 9 (забастовку ведь можно назвать кратковременным нарушением спокойствия?)\n",
    "```\n",
    "\n",
    "**Неверно**\\*: \n",
    "```\n",
    "* 1 ('wrestling', 'slager', 'stun', 'gun', 'break_10', 'away_7', 'shot', 'back' - алгоритм увидел слово \"shot\")\n",
    "* 2 ('break', 'leg' - имелось в виду \"Break a leg!\")\n",
    "* 3 ('favouring', 'hard', 'brexit', 'clean', 'break_7', 'single', 'market' - алгоритм увидел слово \"hard\")\n",
    "* 4 ('grape', 'leaf', 'use', 'break_2', 'torn', 'one' - тут запутался из-за leaf - отнес его к биологии)\n",
    "* 6 ('good_20', 'condition_4', 'break_1', 'left', 'arm', 'bite' - алгоритм увидел слово \"good\")\n",
    "* 7 ('defense', 'break', 'ground', 'million', 'project', 'expected' - просто не нашел ничего общего)\n",
    "* 8 ('joel', 'embiid', 'break', 'handle', 'fake' - просто не нашел ничего общего)\n",
    "```\n",
    "\n",
    "\n",
    "\\* более или менее, т.к. оценка субъективная"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попытка номер 2 - Смотрим на Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_examples( word, sentence ):\n",
    "    \n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    sentence = set(sentence)\n",
    "    \n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        examples = tokenize(' '.join(synset.examples()))\n",
    "        examples = set(examples)\n",
    "        overlap = len(examples & sentence)\n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap = overlap\n",
    "            bestsense = i\n",
    "    \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_test = ['be', 'released', 'known', 'news']\n",
    "# word_test = 'break'\n",
    "# lesk_examples(word_test, sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_words_in_context(sent_test, window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_by_examples(corpus):\n",
    "    dis_corpus = []\n",
    "    # пройдем по корпусу\n",
    "    for text in corpus:\n",
    "        dis_text = []\n",
    "        words_in_context = get_words_in_context(text) \n",
    "        # дизамбигуируем каждой слов\n",
    "        for word, context in words_in_context:\n",
    "            nsense = lesk_examples(word, context)\n",
    "\n",
    "            # если смысл не нулевой - добавим индекс смысла к токену\n",
    "            if nsense > 0:\n",
    "                dis_text.append(word + '_' + str(nsense))\n",
    "            else:\n",
    "                dis_text.append(word)\n",
    "\n",
    "        dis_corpus.append(dis_text)\n",
    "    return dis_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = disambiguate_by_examples(random.sample(corpus_break, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2 = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - break_24 - force out or release suddenly and often violently something pent up',\n",
       " '0 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '1 - break_51 - break down, literally or metaphorically',\n",
       " '2 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " \"3 - break_26 - enter someone's (virtual or real) property in an unauthorized manner, usually with the intent to steal or commit a violent act\",\n",
       " '4 - break_30 - make known to the public information that was previously known only to a few people or that was meant to be kept a secret',\n",
       " '5 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '6 - break_22 - move away or escape suddenly',\n",
       " '7 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '8 - break_46 - discontinue an association or relation; go different ways',\n",
       " '9 - break_1 - an unexpected piece of good luck']"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_all_senses(temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['georgetown', 'mcdonough', 'arena', 'sort', 'intimate', 'flavorful', 'charismatic', 'campus', 'arena', 'make', 'college', 'basketball', 'grand', 'hoya', 'occasional', 'game', 'verizon', 'center', 'tiny', 'little_8', 'cousin', 'nice', 'break_1', 'team', 'big', 'chinatown', 'home', 'logistics', 'aside', 'always', 'thought_1', 'great', 'game_2', 'mcdonough']\n"
     ]
    }
   ],
   "source": [
    "print(temp_2[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты:**\n",
    "```\n",
    "*0 Тут про ломание придметов в ярости - мне кажется, верное значение (break thing anger). Второе неверно (break heart)\n",
    "*1 \"break down, literally\" подходит! текст про то, как ломается лед в океанах\n",
    "*2 Тут подошло бы метафоричное значение предыдущего - про разрушение того, во что кто-то верит (неверно)\n",
    "*3 Что-то про магазины, к сожалению, не ограбление (тогда подошло бы)\n",
    "*4 Ну по сути, breaking news вполне и под это значение подходит - а-ля сенсация\n",
    "*5 Странный текст - возможно про бейсбол, т.к. не понимаю термины. Но смысл определился неверно\n",
    "*6 Неверно - тут про переломную точку\n",
    "*7 Что-то про рабочий коллектив - неверно\n",
    "*8 Неверно - текст про black friday\n",
    "*9 Что-то про футбольную команду - неверно\n",
    "\n",
    "Итого: 3/10 \n",
    "P.S. Тут на самом деле раз на раз не приходится :( если долго подбирать выборку, можно наверняка довести до 6/10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = disambiguate_by_examples(random.sample(corpus_break, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_3 = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - break_23 - scatter or part',\n",
       " \"1 - break_26 - enter someone's (virtual or real) property in an unauthorized manner, usually with the intent to steal or commit a violent act\",\n",
       " '1 - break_4 - a pause from doing something (as work)',\n",
       " '2 - break_29 - surpass in excellence',\n",
       " '3 - break_4 - a pause from doing something (as work)',\n",
       " '3 - break_4 - a pause from doing something (as work)',\n",
       " '4 - break_30 - make known to the public information that was previously known only to a few people or that was meant to be kept a secret',\n",
       " '5 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '6 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '7 - break_22 - move away or escape suddenly',\n",
       " '8 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '9 - break_30 - make known to the public information that was previously known only to a few people or that was meant to be kept a secret']"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_all_senses(temp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ricky', 'martin', 'engaged', 'artist', 'jwan', 'yosef', 'luchina', 'fisher', 'nov', 'share', 'ricky', 'martin', 'appears', 'ellen', 'degeneres', 'show', 'burbank', 'california', 'share', 'ricky', 'martin', 'engaged', 'latin', 'music', 'star', 'proposed', 'jwan', 'yosef', 'artist', 'dating', 'year', 'wednesday', 'break_30', 'news', 'ellen', 'degeneres', 'show', 'conceptual', 'artist', 'collector', 'started', 'looking', 'art', 'saw_15', 'art', 'went', 'crazy_3', 'really', 'love', 'doe', 'really', 'original', 'martin', 'shared', 'contacted', 'collected', 'ellen', 'degeneres', 'joked', 'got', 'engaged', 'martin', 'announced', 'showing', 'ring', 'sweating', 'martin', 'admitted', 'really', 'nervous', 'proposal', 'got_12', 'knee', 'said', 'took_27', 'ring', 'little', 'velvet', 'pouch_1', 'pocket', 'recalled', 'instead', 'saying_1', 'marry_1', 'said', 'got_1', 'something', 'bad_13', 'singer', 'said', 'like', 'yes', 'said', 'want', 'spend', 'life', 'like', 'question_10', 'would', 'marry', 'martin', 'said', 'swept', 'moment_1', 'took', 'minute_1', 'realized_6', 'yosef', 'said', 'yes', 'ricky', 'martin', 'come', 'fortunate_1', 'homosexual', 'man', 'happy', 'deserve', 'degeneres', 'told', 'martin', 'singer', 'raising', 'twin', 'year', 'old_3', 'boy', 'yosef', 'syrian', 'born', 'grew_5', 'sweden', 'life', 'love_5', 'perfect', 'ellen', 'singer', 'said', 'really', 'special_9', 'besides', 'big', 'engagement', 'news', 'former', 'menudo', 'singer', 'currently', 'tour', 'also', 'announced', 'begin_3', 'new', 'vega', 'residency', 'starting_2', 'next', 'year', 'park', 'theater', 'monte', 'carlo', 'share']\n"
     ]
    }
   ],
   "source": [
    "print(temp_3[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты:**\n",
    "```\n",
    "*0 брейкинг ньюс (ронг)\n",
    "*1 О! Кража банковских стредств (ведь чужой аккаунт==счет тоже можно считать property? зато интенция совпадает) +\n",
    "*1_1 Тут имело в виду completely broke (после кражи средств) - неверно\n",
    "*2 Верно! ('expected', 'break_29', 'record_5', 'year') - побить годовые рекорды - чем не excellence +\n",
    "*3 Здесь неоднозначно (ха-ха). Вроде бы и про отпуск (took break), но может и нет +-\n",
    "*4 Брейкинг ньюс! Скажем, плюс\n",
    "*5 Здесь про банкротство (неверно)\n",
    "*6 Что-то про акции (неверно)\n",
    "*7 Тут про рушащиеся мечты (неверно)\n",
    "*8 Хм... А тут про короткий перерыв (short break) в игре - это подходит\n",
    "*9 Брейкинг ньюс! Снова плюс\n",
    "\n",
    "Итого: 5,5/10 (с третьим неоднозначно) \n",
    "```\n",
    "\n",
    "Уже лучше!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попытка номер три - Definitions+Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_defex( word, sentence ):\n",
    "    \n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    sentence = set(sentence)\n",
    "    \n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        examples = tokenize(' '.join(synset.examples()))\n",
    "        definition = tokenize(synset.definition())\n",
    "        defex = set(definition+examples)\n",
    "        overlap = len(defex & sentence)\n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap = overlap\n",
    "            bestsense = i\n",
    "    \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_by_defex(corpus):\n",
    "    dis_corpus = []\n",
    "    # пройдем по корпусу\n",
    "    for text in corpus:\n",
    "        dis_text = []\n",
    "        words_in_context = get_words_in_context(text) \n",
    "        # дизамбигуируем каждой слов\n",
    "        for word, context in words_in_context:\n",
    "            nsense = lesk_defex(word, context)\n",
    "\n",
    "            # если смысл не нулевой - добавим индекс смысла к токену\n",
    "            if nsense > 0:\n",
    "                dis_text.append(word + '_' + str(nsense))\n",
    "            else:\n",
    "                dis_text.append(word)\n",
    "\n",
    "        dis_corpus.append(dis_text)\n",
    "    return dis_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = disambiguate_by_defex(random.sample(corpus_break, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_defex = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '1 - break_11 - (tennis) a score consisting of winning a game when your opponent was serving',\n",
       " '2 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '3 - break_30 - make known to the public information that was previously known only to a few people or that was meant to be kept a secret',\n",
       " '4 - break_47 - assign to a lower position; reduce in rank',\n",
       " '5 - break_1 - an unexpected piece of good luck',\n",
       " '6 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " \"7 - break_2 - (geology) a crack in the earth's crust resulting from the displacement of one side with respect to the other\",\n",
       " \"8 - break_26 - enter someone's (virtual or real) property in an unauthorized manner, usually with the intent to steal or commit a violent act\",\n",
       " '9 - break_30 - make known to the public information that was previously known only to a few people or that was meant to be kept a secret',\n",
       " '9 - break_30 - make known to the public information that was previously known only to a few people or that was meant to be kept a secret']"
      ]
     },
     "execution_count": 639,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_all_senses(temp_defex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'sunday', 'november', 'updated', 'monday', 'november', 'goal', 'latest', 'sfgate', 'homepage', 'click', 'top', 'news', 'around', 'bay', 'area', 'beyond', 'sign', 'newsletter', 'first', 'learn', 'break_30', 'news', 'sign', 'manage', 'profile', 'top', 'page', 'break_30', 'news']\n"
     ]
    }
   ],
   "source": [
    "print(temp_defex[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты:**\n",
    "```\n",
    "0* неверно, тут про breaking campaign (=good)\n",
    "1* не знакома с теннистыми терминами, но выглядит и правда как очки за теннис (только тут table tennis) +\n",
    "2* не разберу значение предложения :( но явно неверно\n",
    "3* что-то про опрос миллиона каталонцев, и это могло бы быть верно, но все кажется, что нет\n",
    "4* скорее всего неверно. Значение забрало из определения, триггер на слово 'lower'\n",
    "5* вау, а тут правда! ('winning', 'big', 'tax', 'break_1', 'trillion', 'foreign', 'profit') +\n",
    "6* тут что-то про ядерный взрыв ('large', 'black', 'mushroom_3', 'cloud', 'rose', 'seaside') - неверно\n",
    "7* неверно, триггер на слово \"one\"\n",
    "8* ой, тут про birth control в honeymoon *awkward* (неверно)\n",
    "9* Брейкинг ньюс! Дважды плюс ++\n",
    "\n",
    "Итого: 4/10\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попытка 4 - нормализация DefEx\n",
    "\n",
    "(на этот способ возлагаются большие надежды. Если оно не сработает, то грустно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_defex(list_defex):\n",
    "    \n",
    "    words = [word.strip(punct) for word in list_defex if word]\n",
    "\n",
    "    words = [lemma.lemmatize(word) for word in words if word]\n",
    "    words = [word for word in words if word and word not in stops]\n",
    "    words = [replaceMultiple(word, for_replacement, '') for word in words if word] \n",
    "    words = [word.strip(punct) for word in words if word]\n",
    "    words = [word for word in words if word and len(word)>2]\n",
    "    words = ['break' if word in break_forms else word for word in words if word] # дополняем лемматизацию\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lesk_defex_norm( word, sentence ):\n",
    "    \n",
    "    bestsense = 0\n",
    "    maxoverlap = 0\n",
    "    sentence = set(sentence)\n",
    "    \n",
    "    for i, synset in enumerate(wn.synsets(word)):\n",
    "        examples = tokenize(' '.join(synset.examples()))\n",
    "        examples = normalize_defex(examples)\n",
    "        definition = tokenize(synset.definition())\n",
    "        definition = normalize_defex(definition)\n",
    "        defex = set(definition+examples)\n",
    "        overlap = len(defex & sentence)\n",
    "        if overlap > maxoverlap:\n",
    "            maxoverlap = overlap\n",
    "            bestsense = i\n",
    "    \n",
    "    return bestsense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_by_defex_norm(corpus):\n",
    "    dis_corpus = []\n",
    "    # пройдем по корпусу\n",
    "    for text in corpus:\n",
    "        dis_text = []\n",
    "        words_in_context = get_words_in_context(text) \n",
    "        # дизамбигуируем каждой слов\n",
    "        for word, context in words_in_context:\n",
    "            nsense = lesk_defex_norm(word, context)\n",
    "\n",
    "            # если смысл не нулевой - добавим индекс смысла к токену\n",
    "            if nsense > 0:\n",
    "                dis_text.append(word + '_' + str(nsense))\n",
    "            else:\n",
    "                dis_text.append(word)\n",
    "\n",
    "        dis_corpus.append(dis_text)\n",
    "    return dis_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = disambiguate_by_defex_norm(random.sample(corpus_break, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_defex_norm = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - break_4 - a pause from doing something (as work)',\n",
       " '1 - break_11 - (tennis) a score consisting of winning a game when your opponent was serving',\n",
       " '2 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '2 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '3 - break_46 - discontinue an association or relation; go different ways',\n",
       " '4 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '5 - break_68 - happen',\n",
       " '6 - break_1 - an unexpected piece of good luck',\n",
       " '6 - break_11 - (tennis) a score consisting of winning a game when your opponent was serving',\n",
       " '7 - break_43 - happen or take place',\n",
       " '8 - break_29 - surpass in excellence',\n",
       " '9 - break_11 - (tennis) a score consisting of winning a game when your opponent was serving']"
      ]
     },
     "execution_count": 678,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_all_senses(temp_defex_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fifth', 'seed_2', 'one', 'six', 'player', 'already', 'qualified', 'season', 'ending', 'final', 'london', 'breezed_1', 'first_8', 'set_28', 'quickly', 'break_11', 'second']\n"
     ]
    }
   ],
   "source": [
    "print(temp_defex_norm[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты:**\n",
    "```\n",
    "0* Тут про таймаут в какой-то игре - подходит +\n",
    "1* Здесь точно про какую-то игру, и break - вероятнее всего про очки +\n",
    "2* Тут про html разметку ('http', 'embed', 'break', 'width', 'height', 'frameborder') - неверно\n",
    "3* Ой, кажется это верно! ('kill', 'friend', 'point_14', 'break_46', 'batman', 'superman', 'dawn', 'justice') про конфликт между супергероями +\n",
    "4* Тут про образование пробок на дорогах - неверно.\n",
    "5* Неверно - тут про heart break (причем метафорично)\n",
    "6* Первое неверно (имелся в виду таймаут). А второе похоже на правду - счет в теннисе (текст про ход игры в теннис). +\n",
    "7* Неверно, триггер на \"take\". Тест про nba taking break for thanksgiving\n",
    "8* Неверно. Здесь зашумляют имена. В идеале их нужно убрать из корпуса.\n",
    "9* Верно, снова про очки в игре (во втором сете) +\n",
    "\n",
    "Итого: 5/10\n",
    "```\n",
    "\n",
    "В принципе, неплохо"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Попытка [-1] - Изменение размера окна"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем поиграться с последним методом (нормализованные description + examples), изменив размер окна на 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_context(words, window=2):\n",
    "    \n",
    "    words_in_context = []\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        left = words[max(0,i-window):i]\n",
    "        right = words[i+1:i+1+window]\n",
    "        target = words[i]\n",
    "        words_in_context.append((target, left+right))\n",
    "        \n",
    "    \n",
    "    return words_in_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = disambiguate_by_defex_norm(random.sample(corpus_break, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_defex_win2 = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - break_21 - act in disregard of laws, rules, contracts, or promises',\n",
       " '1 - break_63 - interrupt the flow of current in',\n",
       " '2 - break_1 - an unexpected piece of good luck',\n",
       " '3 - break_15 - an escape from jail',\n",
       " '4 - break_30 - make known to the public information that was previously known only to a few people or that was meant to be kept a secret',\n",
       " '5 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '6 - break_21 - act in disregard of laws, rules, contracts, or promises',\n",
       " '7 - break_30 - make known to the public information that was previously known only to a few people or that was meant to be kept a secret',\n",
       " '8 - break_12 - an act of delaying or interrupting the continuity',\n",
       " '9 - break_0 - some abrupt occurrence that interrupts an ongoing activity']"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_all_senses(temp_defex_win2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roommate', 'family', 'send', 'condolence', 'card_2', 'also', 'gave_7', 'birthday', 'present_9', 'let', 'stay', 'entirety', 'thanksgiving', 'break', 'family', 'childhood', 'best_9', 'friend', 'clean_3', 'entire', 'house_6', 'death', 'mother', 'also', 'took', 'lunch', 'frozen', 'yogurt', 'summer', 'talk', 'went_4', 'first', 'vacation', 'ever', 'year', 'stayed_3', 'friend_2', 'family', 'showed', 'also', 'embraced_1', 'open_8', 'arm', 'invited', 'visit', 'mexico', 'friend', 'colorado_1', 'friend', 'kenya', 'friend', 'south', 'carolina', 'whose', 'family', 'ready', 'geographical', 'chance']\n"
     ]
    }
   ],
   "source": [
    "print(temp_defex_win2[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты:**\n",
    "```\n",
    "0* Верно (текст про sexual harrassment)\n",
    "1* Верно (республиканец пытается изменить ход событий в конгрессе)\n",
    "2* Неверно (что-то про игрушки)\n",
    "3* Верно (побег из индийской тюрьмы)\n",
    "4* Верно (брейкинг ньюс)\n",
    "5* Неверно (что-то про болельщиков, кажется)\n",
    "6* Верно (какой-то Зума из южной африки был обвинен в нарушении закона)\n",
    "7* Верно (брейкинг ньюс)\n",
    "8* Неверно (что-то про барака обаму на late night show джимми фэллона)\n",
    "9* Неверно (текст про каникулы на день благодарения - thanksgiving break)\n",
    "\n",
    "Итого: 6/10\n",
    "```\n",
    "\n",
    "Пока это лучший результат!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну и последнее - попробуем увеличить размер окна до 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_in_context(words, window=4):\n",
    "    \n",
    "    words_in_context = []\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        left = words[max(0,i-window):i]\n",
    "        right = words[i+1:i+1+window]\n",
    "        target = words[i]\n",
    "        words_in_context.append((target, left+right))\n",
    "        \n",
    "    \n",
    "    return words_in_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = disambiguate_by_defex_norm(random.sample(corpus_break, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_defex_win4 = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - break_1 - an unexpected piece of good luck',\n",
       " '1 - break_47 - assign to a lower position; reduce in rank',\n",
       " '2 - break_4 - a pause from doing something (as work)',\n",
       " '3 - break_46 - discontinue an association or relation; go different ways',\n",
       " '4 - break_30 - make known to the public information that was previously known only to a few people or that was meant to be kept a secret',\n",
       " \"5 - break_26 - enter someone's (virtual or real) property in an unauthorized manner, usually with the intent to steal or commit a violent act\",\n",
       " '6 - break_0 - some abrupt occurrence that interrupts an ongoing activity',\n",
       " '7 - break_21 - act in disregard of laws, rules, contracts, or promises',\n",
       " '8 - break_13 - a sudden dash',\n",
       " '9 - break_30 - make known to the public information that was previously known only to a few people or that was meant to be kept a secret']"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_all_senses(temp_defex_win4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['close', 'image', 'wisconsin', 'wide', 'receiver_5', 'jazz', 'peavy', 'catch', 'front', 'northwestern', 'safety', 'kyle', 'queiro', 'first', 'half_1', 'ncaa', 'college', 'football', 'game', 'evanston', 'ill', 'saturday', 'nov', 'wisconsin', 'wide', 'receiver_5', 'jazz', 'peavy', 'catch', 'front', 'northwestern', 'safety', 'kyle', 'queiro', 'first', 'half_1', 'ncaa', 'college', 'football', 'game', 'evanston', 'ill', 'saturday', 'nov', 'photo', 'david', 'bank', 'image', 'wisconsin', 'running_7', 'back', 'corey', 'clement', 'tackled', 'northwestern', 'linebacker', 'brett', 'walsh', 'first', 'half_1', 'ncaa', 'college', 'football', 'game', 'evanston', 'ill', 'saturday', 'nov', 'wisconsin', 'running_7', 'back', 'corey', 'clement', 'tackled', 'northwestern', 'linebacker', 'brett', 'walsh', 'first', 'half_1', 'ncaa', 'college', 'football', 'game', 'evanston', 'ill', 'saturday', 'nov', 'photo', 'david', 'bank', 'image', 'wisconsin', 'linebacker', 'ryan', 'connelly', 'tackle', 'northwestern', 'running_7', 'back', 'justin', 'jackson_5', 'first', 'half_1', 'ncaa', 'college', 'football', 'game', 'evanston', 'ill', 'saturday', 'nov', 'wisconsin', 'linebacker', 'ryan', 'connelly', 'tackle', 'northwestern', 'running_7', 'back', 'justin', 'jackson_5', 'first', 'half_1', 'ncaa', 'college', 'football', 'game', 'evanston', 'ill', 'saturday', 'nov', 'photo', 'david', 'bank', 'image', 'northwestern', 'quarterback', 'clayton', 'thorson', 'drop_18', 'back', 'wisconsin', 'first', 'half_1', 'ncaa', 'college', 'football', 'game', 'evanston', 'ill', 'saturday', 'nov', 'northwestern', 'quarterback', 'clayton', 'thorson', 'drop_18', 'back', 'wisconsin', 'first', 'half_1', 'ncaa', 'college', 'football', 'game', 'evanston', 'ill', 'saturday', 'nov', 'photo', 'david_2', 'bank', 'image', 'northwestern', 'head', 'coach', 'pat', 'fitzgerald', 'greets', 'team', 'defensive', 'stop', 'wisconsin', 'first_3', 'half_1', 'ncaa', 'college', 'football', 'game', 'evanston', 'ill', 'saturday', 'nov', 'northwestern', 'head', 'coach', 'pat', 'fitzgerald', 'greets', 'team', 'defensive', 'stop', 'wisconsin', 'first_3', 'half_1', 'ncaa', 'college', 'football', 'game', 'evanston', 'ill', 'saturday', 'nov', 'photo', 'david', 'bank', 'clement', 'lead_22', 'wisconsin', 'win', 'northwestern', 'back', 'gallery_1', 'evanston', 'ill', 'building', 'huge', 'victory', 'return', 'top', 'wisconsin', 'badger', 'wanted', 'end_13', 'recent', 'struggle', 'northwestern', 'end', 'also', 'succeeded', 'corey', 'clement', 'ran', 'yard', 'touchdown', 'jazz', 'peavy', 'scored', 'yard', 'run', 'wisconsin', 'beat', 'northwestern', 'saturday', 'conor', 'sheehy', 'forced', 'big_11', 'fumble', 'sacked', 'clayton', 'thorson', 'wildcat', 'big', 'ten', 'threatening', 'early', 'fourth_1', 'quarter_5', 'led_6', 'clements', 'yard', 'run_18', 'making_23', 'point_9', 'game_5', 'preserving', 'third', 'straight', 'win_2', 'badger', 'big_13', 'ten', 'cfp', 'went', 'beating', 'nebraska', 'overtime', 'picking', 'rare', 'win_3', 'northwestern', 'one_1', 'thing', 'really_1', 'wanted', 'beat_31', 'team', 'glad', 'got_8', 'peavy', 'said', 'northwestern', 'six', 'four', 'straight', 'ryan', 'field', 'wisconsin', 'wildcat', 'beat', 'badger', 'six', 'home_14', 'came_1', 'away_2', 'victory', 'camp', 'randall', 'stadium', 'last_8', 'year', 'game', 'three', 'wisconsin', 'touchdown', 'negated', 'call_10', 'official', 'snowball', 'thrown', 'left_19', 'field_20', 'really', 'show_11', 'best_25', 'team', 'today', 'want', 'referee', 'make_14', 'game_2', 'time', 'decision_3', 'end_1', 'said', 'clement', 'carry_6', 'three', 'shy', 'career_2', 'high_7', 'peavy', 'yarder', 'second_4', 'quarter_2', 'first', 'rushing', 'touchdown', 'career', 'also', 'yard_1', 'receiving', 'yard_1', 'punt_2', 'return_11', 'alex', 'hornibrook', 'played', 'game_2', 'completed_3', 'pas', 'yard_1', 'badger', 'rolled', 'yard_1', 'dominating', 'time_5', 'possession_3', 'thought', 'really_3', 'good_4', 'thing', 'coach', 'paul', 'chryst', 'said', 'thought', 'fairly', 'consistent', 'running', 'football', 'defensively', 'know', 'drive_2', 'end_7', 'first', 'half', 'thought', 'lot', 'really_3', 'good_4', 'thing', 'defensively', 'thought', 'really_3', 'good_4', 'northwestern', 'team_2', 'wildcat', 'lost', 'top_3', 'team_2', 'coming_13', 'short_1', 'four', 'point', 'loss_5', 'ohio', 'state', 'last', 'week', 'thorson', 'went_3', 'yard', 'touchdown', 'austin', 'carr', 'added', 'yard_1', 'receiving', 'including', 'yard_1', 'late_10', 'first', 'half', 'justin', 'jackson_3', 'ran_36', 'yard', 'think', 'tired', 'close_1', 'carr', 'said', 'want', 'push_14', 'beat', 'beat', 'two', 'team', 'day', 'especially', 'good_4', 'team', 'wisconsin', 'winning_1', 'ryan', 'wisconsin', 'ryan', 'field_18', 'since', 'several_1', 'big_16', 'play', 'peavy', 'fumble_5', 'recovery', 'cota', 'dixon', 'northwestern', 'threatening', 'early', 'fourth', 'helped_7', 'change', 'wildcat', 'drove_9', 'early', 'quarter', 'got_5', 'pushed', 'back', 'holding_5', 'penalty', 'eric', 'olson', 'yard', 'loss', 'jackson', 'thing', 'got', 'worse_1', 'northwestern', 'next', 'play', 'thorson', 'fumbled_4', 'sacked', 'sheehy', 'dixon', 'returned', 'recovery', 'yard', 'wildcat', 'clement', 'ran_2', 'two', 'point', 'conversion_2', 'run_18', 'alec', 'ingold', 'made', 'left', 'takeaway_1', 'wisconsin', 'badger', 'solidified', 'standing_1', 'top_3', 'taking', 'team', 'playing_1', 'better_3', 'dropping_9', 'first_3', 'two_1', 'game_1', 'northwestern', 'three_2', 'game', 'left', 'wildcat', 'need', 'finish', 'strong_8', 'going_23', 'make_23', 'bowl_6', 'game', 'converting_8', 'northwestern', 'managed_5', 'yard', 'rushing', 'probably', 'help', 'explain', 'wildcat', 'converted', 'third', 'pointafter', 'graphiq', 'number_15', 'change', 'northwestern', 'star', 'linebacker', 'anthony', 'walker', 'wore', 'honor', 'cornerback', 'matthew', 'harris', 'roommate', 'harris', 'got_18', 'hit_1', 'head', 'making_6', 'tackle', 'illinois', 'state', 'sept', 'retired', 'last', 'week', 'walker', 'said', 'back_1', 'wearing', 'final_2', 'word', 'chryst', 'peavy', 'big_16', 'play', 'jazz_3', 'capable', 'need', 'keep', 'maximizing', 'opportunity', 'thought_11', 'part', 'next', 'wisconsin', 'badger', 'get_1', 'bit', 'breather', 'host', 'struggling_1', 'illinois', 'next', 'week', 'northwestern', 'like', 'wisconsin', 'northwestern', 'figure', 'easier_2', 'time', 'next', 'week', 'visit', 'purdue', 'pointafter', 'graphiq', 'latest', 'sfgate', 'homepage', 'click', 'top', 'news', 'around', 'bay', 'area', 'beyond', 'sign_4', 'newsletter', 'first', 'learn', 'break_30', 'news', 'sign', 'manage', 'profile', 'top', 'page', 'latest', 'news']\n"
     ]
    }
   ],
   "source": [
    "print(temp_defex_win4[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Результаты:**\n",
    "```\n",
    "0* В принципе, верно. Говорится о стабилизации рынка после встречи ОПЕК. +\n",
    "1* Неверно, но видно, что триггер на \"lower\"\n",
    "2* Что-то прекратилось делаться первый раз с сентября - в принципе, верно. +\n",
    "3* Опять тот пример про бэтмена с суперменом, определилось правильно, +\n",
    "4* Брейкинг ньюс! +\n",
    "5* Военный в отставке задержан бы вторжение в государственное учреждение (он - rebel - сделал там что-то offensive) +\n",
    "6* Тут про влияние различных веществ на организм. ('cysteine', 'break', 'hangover', 'causing', 'toxin') неверно\n",
    "7* Какой-то штат нарушил бы три государственных закона, если бы сделал что-то (что-то связанное с abortion). +\n",
    "8* AAHH! Вот тут я бы поставила плюс, хотя бы за красивость (метафорично - подходит)! значение - a sudden dash (внезапный всплеск). Текст про WATERgate scandal (тот самый с Никсоном в конце прошлого столетия (Washington, Watergate Hotel), после которого он подал в отставку, опередив импичмент) +\n",
    "9* Брейкинг ньюс! +\n",
    "\n",
    "Итого: 8/10! Абсолютный рекорд!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В итоге, были испробованы множество разных вариантов улучшения алгоритма Леска.\n",
    "\n",
    "На мой взгляд, самый видимиый результат принесла нормализация текстов.\n",
    "\n",
    "Также логично, что увеличение размера окна до 4 при поиске по ```description + examples``` улучшит результат, т.к. текста у нас становится больше, чем при поиске исключительно по ```description``` или ```examples```.\n",
    "\n",
    "Алгоритм часто выбирает неправильный вариант, когда триггерится на совпадающие слова в предложении и в описании (примерах).\n",
    "\n",
    "Алгоритм часто выбирает 0 - так как судя по всему это дефолтное значение, если больше вообще ничего не подходит.\n",
    "\n",
    "``` Выводы: для лучшего результата нужно \n",
    "1) Хорошо нормализовать корпус\n",
    "2) Искать по description + examples\n",
    "3) Увеличить размер окна до 4\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
